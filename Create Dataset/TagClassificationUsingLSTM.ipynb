{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648dc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4ec1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceName = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef96f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\embedding\"):\n",
    "        self.path = path\n",
    "        with open(os.path.join(path, \"vocab.txt\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.merge = dict()\n",
    "        with open(os.path.join(path, \"merge.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            a = f.readlines()[1:]\n",
    "            for index, words in enumerate(a):\n",
    "                words = words.replace(\"\\n\", \"\")\n",
    "                self.merge[tuple(words.strip().split())] = index\n",
    "            \n",
    "        self.reverseVocab = dict()\n",
    "        for i in self.vocab.keys():\n",
    "            self.reverseVocab[self.vocab[i]] = i\n",
    "    \n",
    "    def GetSplitWord(self, txt):        \n",
    "        txt = list(txt)\n",
    "        while(True):            \n",
    "            changeIndex = -1\n",
    "            rank = -1\n",
    "            for index in range(1, len(txt)):\n",
    "                tupl = (txt[index - 1], txt[index])\n",
    "                if(tupl in self.merge and (rank == -1 or (rank != -1 and self.merge[tupl] < rank))):\n",
    "                    changeIndex = index\n",
    "                    rank = self.merge[tupl]\n",
    "            if(changeIndex == -1):\n",
    "                break\n",
    "            txt[changeIndex-1] += txt[changeIndex]\n",
    "            txt.pop(changeIndex)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = txt.replace(\" \", \"Ġ\").replace(\"\\n\", \"Ċ\")\n",
    "        li = self.GetSplitWord(txt)\n",
    "        res = []\n",
    "        for word in li:\n",
    "            if word in self.vocab:     \n",
    "                res.append(self.vocab[word])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, li):\n",
    "        txt = \"\"\n",
    "        for i in li:\n",
    "            txt += self.reverseVocab[i]\n",
    "        return txt.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51cec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightPath = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\weigths\"\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, inputSize = 768, hiddenSize = 2048, lstmHiddenSize = 768):\n",
    "        super().__init__()\n",
    "        self.mlpInputDim = inputSize\n",
    "        self.mlpHiddenDim = hiddenSize\n",
    "        self.lstmHiddenDim = lstmHiddenSize\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lstMBlock = nn.LSTM(input_size=inputSize, hidden_size=lstmHiddenSize, num_layers=3, batch_first=True, device=self.device)\n",
    "        self.linear1 = nn.Linear(in_features=inputSize, out_features=hiddenSize, device=self.device)\n",
    "        self.linear2 = nn.Linear(in_features=hiddenSize, out_features=hiddenSize, device=self.device)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x , (hn, cn) = self.lstMBlock(x)\n",
    "        # print(hn.shape)\n",
    "        return self.linear2(self.relu(self.linear1(hn)))[-1, :, :]\n",
    "\n",
    "class TagClassify(nn.Module):\n",
    "    def __init__(self, numberOfTag = 176, inputSize = 768, hiddenSize = 2048, lstmHiddenSize = 768):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.textEmbd = TextEmbedding(inputSize, hiddenSize, lstmHiddenSize)\n",
    "        self.linear = nn.Linear(in_features=hiddenSize, out_features=numberOfTag, device=self.device)\n",
    "        self.sigmoid = nn.Sigmoid().to(self.device)\n",
    "        self.embd = nn.Embedding(num_embeddings=50257, embedding_dim=768).to(self.device)\n",
    "        self.embd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.wte.weight.npy'))).to(dtype=torch.float32).to(device=self.device))\n",
    "        for param in self.embd.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):   \n",
    "        return self.sigmoid(self.linear(self.textEmbd(self.embd(x)))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9343ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, batchSize = 64):\n",
    "        self.path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\DataSet\\multiTagedData.csv\"\n",
    "        data = pd.read_csv(self.path)\n",
    "        self.poems = list(data.Poem)\n",
    "        self.tags = list(data.Tags)\n",
    "        self.batchSize = batchSize\n",
    "        self.tagIndex = dict()\n",
    "        self.encdr = tokenizer()\n",
    "        self.numberOfTag = self.CreateTag()\n",
    "\n",
    "    def Next(self, index):\n",
    "        max = 0\n",
    "        li = []\n",
    "        batchTag = torch.zeros((self.batchSize, self.numberOfTag))\n",
    "        for i in range(index, min(index + self.batchSize, len(self.poems))):\n",
    "            if(len(self.poems[i].split()) > 300):\n",
    "                continue\n",
    "            encd = self.encdr.encode(self.poems[i])\n",
    "            allTag = self.tags[i].split(\",\")\n",
    "            for tag in allTag:\n",
    "                batchTag[i-self.batchSize][self.tagIndex[tag]] = 1\n",
    "            if(len(encd) > max):\n",
    "                max = len(encd)\n",
    "            li.append(encd)        \n",
    "        seqPoem = torch.full((self.batchSize, max), 50256)\n",
    "        for i in range(len(li)):\n",
    "            seqPoem[i,:len(li[i])] = torch.tensor(li[i])\n",
    "        return seqPoem, batchTag\n",
    "\n",
    "    def CreateTag(self):\n",
    "        index = 0\n",
    "        for tag in self.tags:\n",
    "            allTag = tag.split(\",\")\n",
    "            for eachTag in allTag:\n",
    "                if(eachTag not in self.tagIndex):\n",
    "                    self.tagIndex[eachTag] = index\n",
    "                    index += 1\n",
    "        return len(self.tagIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85a132ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TagClassify()\n",
    "tkn = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa76a15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 176])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(tkn.encode(\"why are we getting this error\")).to(model.device)\n",
    "a = a.unsqueeze(0)\n",
    "model(a).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
