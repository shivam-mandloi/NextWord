{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8214785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "663976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceName = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f842abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "        => In one note all the math is explained\n",
    "    \"\"\"\n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        # total 768 X 64 | 64 is query, key and value dim\n",
    "        # 2304 / 64 = 36 | 3 q, k, v and total 12 head\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attention = nn.Parameter(torch.zeros((768, 2304), device=self.device)) # weight\n",
    "        self.attentionBias = nn.Parameter(torch.zeros(2304, device=self.device)) # Bias\n",
    "        self.attentionProj = nn.Parameter(torch.zeros((768, 768), device=self.device))\n",
    "        self.attentionProjBias = nn.Parameter(torch.zeros(768, device=self.device))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b X m X n\n",
    "        # mask = b X m X m\n",
    "        # b = batch size | m = seq len | n = input dim (768)\n",
    "        B, T, C = x.shape\n",
    "        x = x @ self.attention + self.attentionBias # (result) x = b X m X 2304\n",
    "        Q, K, V = x.split(768, -1) # Q, K, V = b X m X 768\n",
    "        batch, seqLen, hiddeDim = Q.shape\n",
    "        \n",
    "        # Q, K, V = b X head number X m X hidden dim\n",
    "        Q = Q.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        K = K.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        V = V.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        \n",
    "        score = (Q @ K.permute(0, 1, 3, 2)) / 8 # root(64) = 8\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        sf = self.softmax(score)\n",
    "        x = (sf @ V).permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        return x @ self.attentionProj + self.attentionProjBias\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.weight1 = nn.Parameter(torch.zeros((768, 3072), device=self.device))\n",
    "        self.bias1 = nn.Parameter(torch.zeros(3072, device=self.device))\n",
    "        self.weight2 = nn.Parameter(torch.zeros((3072, 768), device=self.device))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(768, device=self.device))\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        return self.gelu(x @ self.weight1 + self.bias1) @ self.weight2 + self.bias2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention().to(self.device)\n",
    "        self.mlp = MLP().to(self.device)\n",
    "        self.layerNorm1 = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm2 = nn.LayerNorm(768).to(self.device)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        t = self.layerNorm1(x)\n",
    "        t = self.attn(t, mask)\n",
    "        t = self.dropout1(t)\n",
    "        x = x + t\n",
    "        # according to diagram the value is stored in x in below layerNorm2\n",
    "        # but diagram is wrong, believe me I waste 2 days on this.\n",
    "        t = self.layerNorm2(x)\n",
    "        t = self.mlp(t)\n",
    "        t = self.dropout2(t)\n",
    "        x = x + t\n",
    "        return x\n",
    "\n",
    "\n",
    "class gpt2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embd = nn.Embedding(num_embeddings=50257, embedding_dim=768).to(self.device)\n",
    "        self.positionEmbd = nn.Embedding(num_embeddings=1024, embedding_dim=768).to(self.device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Transformer() for _ in range(12)\n",
    "        ]).to(self.device)\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(768).to(self.device)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b X m \n",
    "        # b = batch size, m = sequence len\n",
    "        batchSize, seqLen = x.shape\n",
    "        posInput = torch.arange(0, seqLen).expand(batchSize, seqLen).to(self.device)\n",
    "        embdX = self.embd(x)        \n",
    "        posEmbdX = self.positionEmbd(posInput)        \n",
    "\n",
    "        x = embdX + posEmbdX\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x @ self.embd.weight.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1bfdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGPT2Model():\n",
    "    layerName = ['transformer.h.k.ln_1.weight.npy',\n",
    "            'transformer.h.k.ln_1.bias.npy',\n",
    "            'transformer.h.k.attn.c_attn.weight.npy',\n",
    "            'transformer.h.k.attn.c_attn.bias.npy',\n",
    "            'transformer.h.k.attn.c_proj.weight.npy',\n",
    "            'transformer.h.k.attn.c_proj.bias.npy',\n",
    "            'transformer.h.k.ln_2.weight.npy',\n",
    "            'transformer.h.k.ln_2.bias.npy',\n",
    "            'transformer.h.k.mlp.c_fc.weight.npy',\n",
    "            'transformer.h.k.mlp.c_fc.bias.npy',\n",
    "            'transformer.h.k.mlp.c_proj.weight.npy',\n",
    "            'transformer.h.k.mlp.c_proj.bias.npy']\n",
    "    path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\weigths\"\n",
    "    gptModel = gpt2()\n",
    "    gptModel.embd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.wte.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    gptModel.positionEmbd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.wpe.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    for index, layer in enumerate(gptModel.layers):\n",
    "        layer.layerNorm1.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[0].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.layerNorm1.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[1].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.attn.attention = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[2].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.attn.attentionBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[3].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.attn.attentionProj = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[4].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.attn.attentionProjBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[5].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.layerNorm2.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[6].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.layerNorm2.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[7].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.mlp.weight1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[8].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.mlp.bias1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[9].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.mlp.weight2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[10].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "        layer.mlp.bias2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[11].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    gptModel.layerNorm.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.ln_f.bias.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    gptModel.layerNorm.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.ln_f.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    return gptModel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64a00c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\embedding\"):\n",
    "        self.path = path\n",
    "        with open(os.path.join(path, \"vocab.txt\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.merge = dict()\n",
    "        with open(os.path.join(path, \"merge.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            a = f.readlines()[1:]\n",
    "            for index, words in enumerate(a):\n",
    "                words = words.replace(\"\\n\", \"\")\n",
    "                self.merge[tuple(words.strip().split())] = index\n",
    "            \n",
    "        self.reverseVocab = dict()\n",
    "        for i in self.vocab.keys():\n",
    "            self.reverseVocab[self.vocab[i]] = i\n",
    "    \n",
    "    def GetSplitWord(self, txt):        \n",
    "        txt = list(txt)\n",
    "        while(True):            \n",
    "            changeIndex = -1\n",
    "            rank = -1\n",
    "            for index in range(1, len(txt)):\n",
    "                tupl = (txt[index - 1], txt[index])\n",
    "                if(tupl in self.merge and (rank == -1 or (rank != -1 and self.merge[tupl] < rank))):\n",
    "                    changeIndex = index\n",
    "                    rank = self.merge[tupl]\n",
    "            if(changeIndex == -1):\n",
    "                break\n",
    "            txt[changeIndex-1] += txt[changeIndex]\n",
    "            txt.pop(changeIndex)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = txt.replace(\" \", \"Ġ\")\n",
    "        li = self.GetSplitWord(txt)\n",
    "        res = []\n",
    "        for word in li:            \n",
    "            res.append(self.vocab[word])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, li):\n",
    "        txt = \"\"\n",
    "        for i in li:\n",
    "            txt += self.reverseVocab[i]\n",
    "        return txt.replace(\"Ġ\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3fb5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMask(encd:list)->torch.tensor:\n",
    "    n = len(encd)\n",
    "    mask = torch.ones((n,n), device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\"))\n",
    "    mask = mask.tril()\n",
    "    return mask.unsqueeze(0)\n",
    "\n",
    "def SelectNextWord(prob):\n",
    "    sf = nn.Softmax(dim=-1)\n",
    "    prob = prob[-1] / 0.98\n",
    "    prob = sf(prob)\n",
    "    return torch.multinomial(prob, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b06a560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a71c8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22850, 502]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn.encode(\"why me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55d0d377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt2(\n",
       "  (embd): Embedding(50257, 768)\n",
       "  (positionEmbd): Embedding(1024, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Transformer(\n",
       "      (attn): Attention(\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "      )\n",
       "      (layerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptModel = CreateGPT2Model()\n",
    "gptModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64037c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_7096\\3598555013.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here look into it. You see, I told you after all's said and done, how you could go back to Sweet Tooth in a day and make an address in New York is unbelievable. I've never had a major disagreement about films or music before.\n"
     ]
    }
   ],
   "source": [
    "sent = \"Here look\"\n",
    "encd = tkn.encode(sent)\n",
    "itera = 1\n",
    "while(True):\n",
    "    x = torch.tensor(encd, device=deviceName)\n",
    "    mask = CreateMask(encd)\n",
    "    a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n",
    "    encd.append(SelectNextWord(a))\n",
    "    itera += 1\n",
    "    if(itera > 50):\n",
    "        break\n",
    "print(tkn.decode(encd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
