{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8214785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceName = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f842abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightPath = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\weigths\"\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "        => In one note all the math is explained\n",
    "    \"\"\"\n",
    "    def __init__(self, index):        \n",
    "        super().__init__()\n",
    "        # total 768 X 64 | 64 is query, key and value dim\n",
    "        # 2304 / 64 = 36 | 3 q, k, v and total 12 head\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attention = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_attn.weight.npy\"))).to(dtype=torch.float32).to(device=self.device)) # weight\n",
    "        self.attentionBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_attn.bias.npy\"))).to(dtype=torch.float32).to(device=self.device)) # Bias\n",
    "        self.attentionProj = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_proj.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.attentionProjBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_proj.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b X m X n\n",
    "        # mask = b X m X m\n",
    "        # b = batch size | m = seq len | n = input dim (768)\n",
    "        B, T, C = x.shape\n",
    "        x = x @ self.attention + self.attentionBias # (result) x = b X m X 2304\n",
    "        Q, K, V = x.split(768, -1) # Q, K, V = b X m X 768\n",
    "        batch, seqLen, hiddeDim = Q.shape\n",
    "        \n",
    "        # Q, K, V = b X head number X m X hidden dim\n",
    "        Q = Q.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        K = K.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        V = V.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        \n",
    "        score = (Q @ K.permute(0, 1, 3, 2)) / 8 # root(64) = 8\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        sf = self.softmax(score)\n",
    "        x = (sf @ V).permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        return x @ self.attentionProj + self.attentionProjBias\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.weight1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_fc.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.bias1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_fc.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.weight2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_proj.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.bias2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_proj.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        return self.gelu(x @ self.weight1 + self.bias1) @ self.weight2 + self.bias2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention(index).to(self.device)\n",
    "        self.mlp = MLP(index).to(self.device)\n",
    "        self.layerNorm1 = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm2 = nn.LayerNorm(768).to(self.device)        \n",
    "        self.layerNorm1.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_1.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm1.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_1.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm2.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_2.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm2.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_2.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        t = self.layerNorm1(x)\n",
    "        t = self.attn(t, mask)\n",
    "        t = self.dropout1(t)\n",
    "        x = x + t\n",
    "        # according to diagram the value is stored in x in below layerNorm2\n",
    "        # but diagram is wrong, believe me I waste 2 days on this.\n",
    "        t = self.layerNorm2(x)\n",
    "        t = self.mlp(t)\n",
    "        t = self.dropout2(t)\n",
    "        x = x + t\n",
    "        return x\n",
    "\n",
    "\n",
    "class gpt2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embd = nn.Embedding(num_embeddings=50257, embedding_dim=768).to(self.device)\n",
    "        self.embd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.wte.weight.npy'))).to(dtype=torch.float32).to(device=self.device)) # copy embedding matrix\n",
    "        self.positionEmbd = nn.Embedding(num_embeddings=1024, embedding_dim=768).to(self.device)\n",
    "        self.positionEmbd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.wpe.weight.npy'))).to(dtype=torch.float32).to(device=self.device))  # copy embedding matrix\n",
    "        self.layers = nn.ModuleList([\n",
    "            Transformer(index) for index in range(12)\n",
    "        ]).to(self.device)\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.ln_f.bias.npy'))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.ln_f.weight.npy'))).to(dtype=torch.float32).to(device=self.device))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b X m \n",
    "        # b = batch size, m = sequence len\n",
    "        batchSize, seqLen = x.shape\n",
    "        posInput = torch.arange(0, seqLen).expand(batchSize, seqLen).to(self.device)\n",
    "        embdX = self.embd(x)        \n",
    "        posEmbdX = self.positionEmbd(posInput)        \n",
    "\n",
    "        x = embdX + posEmbdX\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x @ self.embd.weight.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "64a00c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\embedding\"):\n",
    "        self.path = path\n",
    "        with open(os.path.join(path, \"vocab.txt\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.merge = dict()\n",
    "        with open(os.path.join(path, \"merge.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            a = f.readlines()[1:]\n",
    "            for index, words in enumerate(a):\n",
    "                words = words.replace(\"\\n\", \"\")\n",
    "                self.merge[tuple(words.strip().split())] = index\n",
    "            \n",
    "        self.reverseVocab = dict()\n",
    "        for i in self.vocab.keys():\n",
    "            self.reverseVocab[self.vocab[i]] = i\n",
    "    \n",
    "    def GetSplitWord(self, txt):        \n",
    "        txt = list(txt)\n",
    "        while(True):            \n",
    "            changeIndex = -1\n",
    "            rank = -1\n",
    "            for index in range(1, len(txt)):\n",
    "                tupl = (txt[index - 1], txt[index])\n",
    "                if(tupl in self.merge and (rank == -1 or (rank != -1 and self.merge[tupl] < rank))):\n",
    "                    changeIndex = index\n",
    "                    rank = self.merge[tupl]\n",
    "            if(changeIndex == -1):\n",
    "                break\n",
    "            txt[changeIndex-1] += txt[changeIndex]\n",
    "            txt.pop(changeIndex)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = txt.replace(\" \", \"Ġ\").replace(\"\\n\", \"Ċ\")\n",
    "        li = self.GetSplitWord(txt)\n",
    "        res = []\n",
    "        for word in li:            \n",
    "            res.append(self.vocab[word])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, li):\n",
    "        txt = \"\"\n",
    "        for i in li:\n",
    "            txt += self.reverseVocab[i]\n",
    "        return txt.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3fb5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMask(encd:list)->torch.tensor:\n",
    "    n = len(encd)\n",
    "    mask = torch.ones((n,n), device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\"))\n",
    "    mask = mask.tril()\n",
    "    return mask.unsqueeze(0)\n",
    "\n",
    "def SelectNextWord(prob):\n",
    "    sf = nn.Softmax(dim=-1)\n",
    "    prob = prob[-1] / 0.8\n",
    "    prob = sf(prob)\n",
    "    return torch.multinomial(prob, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b06a560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55d0d377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt2(\n",
       "  (embd): Embedding(50257, 768)\n",
       "  (positionEmbd): Embedding(1024, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Transformer(\n",
       "      (attn): Attention(\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "      )\n",
       "      (layerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptModel = gpt2()\n",
    "gptModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "64037c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_20208\\3215350776.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you fucking laxer.\" He turned his head from one side to the other with a mixture of confusion and disbelief at his own stupidity.\n",
      "\n",
      "He was not listening. He did not want to listen or even think about it. He was simply conveying his ignorance to the people who know how to do it. They just had to do it a certain way. Otherwise, he would be trying to mislead them.\n",
      "\n",
      "-----\n",
      "\n",
      "The others who had been expecting this happened to be a subway train.\n"
     ]
    }
   ],
   "source": [
    "sent = \"you fucking\"\n",
    "encd = tkn.encode(sent)\n",
    "itera = 1\n",
    "while(True):\n",
    "    x = torch.tensor(encd, device=deviceName)\n",
    "    mask = CreateMask(encd)\n",
    "    a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n",
    "    encd.append(SelectNextWord(a))\n",
    "    itera += 1\n",
    "    if(itera > 100):\n",
    "        break\n",
    "print(tkn.decode(encd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
