{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfc5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "deviceName = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9937ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "        => In one note all the math is explained\n",
    "    \"\"\"\n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        # total 768 X 64 | 64 is query, key and value dim\n",
    "        # 2304 / 64 = 36 | 3 q, k, v and total 12 head\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attention = nn.Parameter(torch.zeros((768, 2304), device=self.device)) # weight\n",
    "        self.attentionBias = nn.Parameter(torch.zeros(2304, device=self.device)) # Bias\n",
    "        self.attentionProj = nn.Parameter(torch.zeros((768, 768), device=self.device))\n",
    "        self.attentionProjBias = nn.Parameter(torch.zeros(768, device=self.device))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        # Fine Tune\n",
    "        self.A = nn.Parameter(torch.rand(768, 128))\n",
    "        self.B = nn.Parameter(torch.rand(128, 2304))\n",
    "\n",
    "        # freeze the weight\n",
    "        self.attention.requires_grad = False\n",
    "        self.attentionBias.requires_grad = False\n",
    "        self.attentionProj.requires_grad = False\n",
    "        self.attentionProjBias.requires_grad = False\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b X m X n\n",
    "        # mask = b X m X m\n",
    "        # b = batch size | m = seq len | n = input dim (768)\n",
    "        B, T, C = x.shape\n",
    "        x = (x @ self.attention + self.attentionBias) + (x @ self.A @ self.B) # (result) x = b X m X 2304\n",
    "        Q, K, V = x.split(768, -1) # Q, K, V = b X m X 768\n",
    "        batch, seqLen, hiddeDim = Q.shape\n",
    "        \n",
    "        # Q, K, V = b X head number X m X hidden dim\n",
    "        Q = Q.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        K = K.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        V = V.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        \n",
    "        score = (Q @ K.permute(0, 1, 3, 2)) / 8 # root(64) = 8\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        sf = self.softmax(score)\n",
    "        x = (sf @ V).permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        return x @ self.attentionProj + self.attentionProjBias\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.weight1 = nn.Parameter(torch.zeros((768, 3072), device=self.device))\n",
    "        self.bias1 = nn.Parameter(torch.zeros(3072, device=self.device))\n",
    "        self.weight2 = nn.Parameter(torch.zeros((3072, 768), device=self.device))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(768, device=self.device))\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "        # freeze the weight\n",
    "        self.weight1.requires_grad = False\n",
    "        self.bias1.requires_grad = False\n",
    "        self.weight2.requires_grad = False\n",
    "        self.weight2.requires_grad = False\n",
    "        self.bias2.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        return self.gelu(x @ self.weight1 + self.bias1) @ self.weight2 + self.bias2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention().to(self.device)\n",
    "        self.mlp = MLP().to(self.device)\n",
    "        self.layerNorm1 = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm2 = nn.LayerNorm(768).to(self.device)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        # freeze the weight\n",
    "        self.layerNorm1.weight.requires_grad = False\n",
    "        self.layerNorm1.bias.requires_grad = False\n",
    "        self.layerNorm2.weight.requires_grad = False\n",
    "        self.layerNorm2.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        t = self.layerNorm1(x)\n",
    "        t = self.attn(t, mask)\n",
    "        t = self.dropout1(t)\n",
    "        x = x + t\n",
    "        # according to diagram the value is stored in x in below layerNorm2\n",
    "        # but diagram is wrong, believe me I waste 2 days on this.\n",
    "        t = self.layerNorm2(x)\n",
    "        t = self.mlp(t)\n",
    "        t = self.dropout2(t)\n",
    "        x = x + t\n",
    "        return x\n",
    "\n",
    "\n",
    "class gpt2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embd = nn.Embedding(num_embeddings=50257, embedding_dim=768).to(self.device)\n",
    "        self.positionEmbd = nn.Embedding(num_embeddings=1024, embedding_dim=768).to(self.device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Transformer() for _ in range(12)\n",
    "        ]).to(self.device)\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(768).to(self.device)\n",
    "\n",
    "        # freeze the weight\n",
    "        self.embd.weight.requires_grad = False        \n",
    "        self.positionEmbd.weight.requires_grad = False\n",
    "        self.layerNorm.weight.requires_grad = False\n",
    "        self.layerNorm.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b X m \n",
    "        # b = batch size, m = sequence len\n",
    "        batchSize, seqLen = x.shape\n",
    "        posInput = torch.arange(0, seqLen).expand(batchSize, seqLen).to(self.device)\n",
    "        embdX = self.embd(x)        \n",
    "        posEmbdX = self.positionEmbd(posInput)        \n",
    "\n",
    "        x = embdX + posEmbdX\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x @ self.embd.weight.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083a03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGPT2Model():\n",
    "    layerName = ['transformer.h.k.ln_1.weight.npy',\n",
    "            'transformer.h.k.ln_1.bias.npy',\n",
    "            'transformer.h.k.attn.c_attn.weight.npy',\n",
    "            'transformer.h.k.attn.c_attn.bias.npy',\n",
    "            'transformer.h.k.attn.c_proj.weight.npy',\n",
    "            'transformer.h.k.attn.c_proj.bias.npy',\n",
    "            'transformer.h.k.ln_2.weight.npy',\n",
    "            'transformer.h.k.ln_2.bias.npy',\n",
    "            'transformer.h.k.mlp.c_fc.weight.npy',\n",
    "            'transformer.h.k.mlp.c_fc.bias.npy',\n",
    "            'transformer.h.k.mlp.c_proj.weight.npy',\n",
    "            'transformer.h.k.mlp.c_proj.bias.npy']\n",
    "    path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\weigths\"\n",
    "    gptModel = gpt2()\n",
    "    gptModel.embd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.wte.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    gptModel.positionEmbd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.wpe.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device))\n",
    "    for index, layer in enumerate(gptModel.layers):\n",
    "        layer.layerNorm1.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[0].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.layerNorm1.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[1].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.attn.attention = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[2].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.attn.attentionBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[3].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.attn.attentionProj = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[4].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.attn.attentionProjBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[5].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.layerNorm2.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[6].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.layerNorm2.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[7].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.mlp.weight1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[8].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.mlp.bias1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[9].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.mlp.weight2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[10].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "        layer.mlp.bias2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, layerName[11].replace(\"k\", str(index))))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "    gptModel.layerNorm.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.ln_f.bias.npy'))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "    gptModel.layerNorm.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(path, 'transformer.ln_f.weight.npy'))).to(dtype=torch.float32).to(device=gptModel.device), requires_grad=False)\n",
    "    return gptModel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33ca1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\embedding\"):\n",
    "        self.path = path\n",
    "        with open(os.path.join(path, \"vocab.txt\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.merge = dict()\n",
    "        with open(os.path.join(path, \"merge.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            a = f.readlines()[1:]\n",
    "            for index, words in enumerate(a):\n",
    "                words = words.replace(\"\\n\", \"\")\n",
    "                self.merge[tuple(words.strip().split())] = index\n",
    "            \n",
    "        self.reverseVocab = dict()\n",
    "        for i in self.vocab.keys():\n",
    "            self.reverseVocab[self.vocab[i]] = i\n",
    "    \n",
    "    def GetSplitWord(self, txt):        \n",
    "        txt = list(txt)\n",
    "        while(True):            \n",
    "            changeIndex = -1\n",
    "            rank = -1\n",
    "            for index in range(1, len(txt)):\n",
    "                tupl = (txt[index - 1], txt[index])\n",
    "                if(tupl in self.merge and (rank == -1 or (rank != -1 and self.merge[tupl] < rank))):\n",
    "                    changeIndex = index\n",
    "                    rank = self.merge[tupl]\n",
    "            if(changeIndex == -1):\n",
    "                break\n",
    "            txt[changeIndex-1] += txt[changeIndex]\n",
    "            txt.pop(changeIndex)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = txt.replace(\" \", \"Ġ\")\n",
    "        li = self.GetSplitWord(txt)\n",
    "        res = []\n",
    "        for word in li:        \n",
    "            if(word in self.vocab):    \n",
    "                res.append(self.vocab[word])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, li):\n",
    "        txt = \"\"\n",
    "        for i in li:\n",
    "            txt += self.reverseVocab[i]\n",
    "        return txt.replace(\"Ġ\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145bfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMask(encd:list)->torch.tensor:\n",
    "    n = len(encd)\n",
    "    mask = torch.ones((n,n), device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\"))\n",
    "    mask = mask.tril()\n",
    "    return mask.unsqueeze(0)\n",
    "\n",
    "def SelectNextWord(prob):\n",
    "    sf = nn.Softmax(dim=-1)\n",
    "    prob = prob[-1] / 0.98\n",
    "    prob = sf(prob)\n",
    "    return torch.multinomial(prob, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6644034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embd.weight torch.Size([50257, 768])\n",
      "positionEmbd.weight torch.Size([1024, 768])\n",
      "layers.0.attn.A torch.Size([768, 128])\n",
      "layers.0.attn.B torch.Size([128, 2304])\n",
      "layers.1.attn.A torch.Size([768, 128])\n",
      "layers.1.attn.B torch.Size([128, 2304])\n",
      "layers.2.attn.A torch.Size([768, 128])\n",
      "layers.2.attn.B torch.Size([128, 2304])\n",
      "layers.3.attn.A torch.Size([768, 128])\n",
      "layers.3.attn.B torch.Size([128, 2304])\n",
      "layers.4.attn.A torch.Size([768, 128])\n",
      "layers.4.attn.B torch.Size([128, 2304])\n",
      "layers.5.attn.A torch.Size([768, 128])\n",
      "layers.5.attn.B torch.Size([128, 2304])\n",
      "layers.6.attn.A torch.Size([768, 128])\n",
      "layers.6.attn.B torch.Size([128, 2304])\n",
      "layers.7.attn.A torch.Size([768, 128])\n",
      "layers.7.attn.B torch.Size([128, 2304])\n",
      "layers.8.attn.A torch.Size([768, 128])\n",
      "layers.8.attn.B torch.Size([128, 2304])\n",
      "layers.9.attn.A torch.Size([768, 128])\n",
      "layers.9.attn.B torch.Size([128, 2304])\n",
      "layers.10.attn.A torch.Size([768, 128])\n",
      "layers.10.attn.B torch.Size([128, 2304])\n",
      "layers.11.attn.A torch.Size([768, 128])\n",
      "layers.11.attn.B torch.Size([128, 2304])\n"
     ]
    }
   ],
   "source": [
    "tkn = tokenizer()\n",
    "gptModel = CreateGPT2Model()\n",
    "for name, p in gptModel.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(name, p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7edd1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        dataSet = pd.read_csv(r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\DataSet\\PeomWithTagDataSet.csv\")\n",
    "        self.poem = [' '.join(i.split()) for i in list(dataSet.Poem)]\n",
    "        self.tags = [' '.join(i.split()) for i in list(dataSet.Tags)]\n",
    "        self.model = CreateGPT2Model()\n",
    "        self.tkn = tokenizer()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # training \n",
    "        self.batchSize = 32\n",
    "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=1e-3)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def CreateData(self, index):\n",
    "        encode = tkn.encode(self.poem[index])\n",
    "        tags = tkn.encode(self.tags[index])\n",
    "        return encode, tags\n",
    "\n",
    "    def Train(self, epoch = 10):\n",
    "        for epc in range(epoch):\n",
    "            for i in range(0, len(self.poem), self.batchSize):\n",
    "                data, tags = self.CreateData(i)\n",
    "                totalLoss = 0\n",
    "                for k in range(len(data)):\n",
    "                    mask = CreateMask(tags)\n",
    "                    target = torch.tensor(data[k], device=self.device).unsqueeze(0)\n",
    "                    logit = self.model(torch.tensor(tags, device=self.device).view(1, len(tags)), mask)\n",
    "                    loss = self.loss_fn(logit[:,-1,:], target)\n",
    "                    totalLoss += loss\n",
    "                    tags.append(data[k])\n",
    "                print(f\"Epoch: {epc + 1}/{epoch} | Poem No.: {i + 1}/{len(self.poem)} | Loss: {totalLoss.item()}\")\n",
    "                totalLoss /= len(data)\n",
    "                totalLoss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                break\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296a1361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "tr = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338806be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Poem No.: 1/13854 | Loss: 2446.365234375\n"
     ]
    }
   ],
   "source": [
    "tr.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9860ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_14144\\3598555013.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here lookhogLabourleanspick Mastery Breaker myster Yard Labourgive Album Breakertle romanticWomenwomenInterestoustPack LabourWomenwomenleans intimate TraderShip MasteryroundLabourLady Yardem Trader Contest Gentle tonShip YardDist weightsleansentleUMEem Warehouseem TraderWomenTrust myster\n"
     ]
    }
   ],
   "source": [
    "sent = \"Here look\"\n",
    "encd = tkn.encode(sent)\n",
    "itera = 1\n",
    "while(True):\n",
    "    x = torch.tensor(encd, device=deviceName)\n",
    "    mask = CreateMask(encd)\n",
    "    a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n",
    "    encd.append(SelectNextWord(a))\n",
    "    itera += 1\n",
    "    if(itera > 50):\n",
    "        break\n",
    "print(tkn.decode(encd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cf8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
