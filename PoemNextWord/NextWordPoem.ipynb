{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cfc5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "deviceName = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightPath = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\weigths\"\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "        => In one note all the math is explained\n",
    "    \"\"\"\n",
    "    def __init__(self, index):        \n",
    "        super().__init__()\n",
    "        # total 768 X 64 | 64 is query, key and value dim\n",
    "        # 2304 / 64 = 36 | 3 q, k, v and total 12 head\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attention = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_attn.weight.npy\"))).to(dtype=torch.float32).to(device=self.device)) # weight\n",
    "        self.attentionBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_attn.bias.npy\"))).to(dtype=torch.float32).to(device=self.device)) # Bias\n",
    "        self.attentionProj = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_proj.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.attentionProjBias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.attn.c_proj.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b X m X n\n",
    "        # mask = b X m X m\n",
    "        # b = batch size | m = seq len | n = input dim (768)\n",
    "        B, T, C = x.shape\n",
    "        x = x @ self.attention + self.attentionBias # (result) x = b X m X 2304\n",
    "        Q, K, V = x.split(768, -1) # Q, K, V = b X m X 768\n",
    "        batch, seqLen, hiddeDim = Q.shape\n",
    "        \n",
    "        # Q, K, V = b X head number X m X hidden dim\n",
    "        Q = Q.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        K = K.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        V = V.view((batch, seqLen, 12, 64)).permute(0, 2, 1, 3)\n",
    "        \n",
    "        score = (Q @ K.permute(0, 1, 3, 2)) / 8 # root(64) = 8\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        sf = self.softmax(score)\n",
    "        x = (sf @ V).permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        return x @ self.attentionProj + self.attentionProjBias\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.weight1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_fc.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.bias1 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_fc.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.weight2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_proj.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.bias2 = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.mlp.c_proj.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        return self.gelu(x @ self.weight1 + self.bias1) @ self.weight2 + self.bias2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention(index).to(self.device)\n",
    "        self.mlp = MLP(index).to(self.device)\n",
    "        self.layerNorm1 = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm2 = nn.LayerNorm(768).to(self.device)        \n",
    "        self.layerNorm1.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_1.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm1.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_1.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm2.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_2.weight.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm2.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, f\"transformer.h.{index}.ln_2.bias.npy\"))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x = b  X m X 768\n",
    "        # b = batch size, m = sequence len\n",
    "        t = self.layerNorm1(x)\n",
    "        t = self.attn(t, mask)\n",
    "        t = self.dropout1(t)\n",
    "        x = x + t\n",
    "        # according to diagram the value is stored in x in below layerNorm2\n",
    "        # but diagram is wrong, believe me I waste 2 days on this.\n",
    "        t = self.layerNorm2(x)\n",
    "        t = self.mlp(t)\n",
    "        t = self.dropout2(t)\n",
    "        x = x + t\n",
    "        return x\n",
    "\n",
    "\n",
    "class gpt2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embd = nn.Embedding(num_embeddings=50257, embedding_dim=768).to(self.device)\n",
    "        self.embd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.wte.weight.npy'))).to(dtype=torch.float32).to(device=self.device)) # copy embedding matrix\n",
    "        self.positionEmbd = nn.Embedding(num_embeddings=1024, embedding_dim=768).to(self.device)\n",
    "        self.positionEmbd.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.wpe.weight.npy'))).to(dtype=torch.float32).to(device=self.device))  # copy embedding matrix\n",
    "        self.layers = nn.ModuleList([\n",
    "            Transformer(index) for index in range(12)\n",
    "        ]).to(self.device)\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(768).to(self.device)\n",
    "        self.layerNorm.bias = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.ln_f.bias.npy'))).to(dtype=torch.float32).to(device=self.device))\n",
    "        self.layerNorm.weight = nn.Parameter(torch.from_numpy(np.load(os.path.join(weightPath, 'transformer.ln_f.weight.npy'))).to(dtype=torch.float32).to(device=self.device))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # b X m \n",
    "        # b = batch size, m = sequence len\n",
    "        batchSize, seqLen = x.shape\n",
    "        posInput = torch.arange(0, seqLen).expand(batchSize, seqLen).to(self.device)\n",
    "        embdX = self.embd(x)        \n",
    "        posEmbdX = self.positionEmbd(posInput)        \n",
    "\n",
    "        x = embdX + posEmbdX\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layerNorm(x)\n",
    "        \n",
    "        return x @ self.embd.weight.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, path = r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\embedding\"):\n",
    "        self.path = path\n",
    "        with open(os.path.join(path, \"vocab.txt\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.merge = dict()\n",
    "        with open(os.path.join(path, \"merge.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            a = f.readlines()[1:]\n",
    "            for index, words in enumerate(a):\n",
    "                words = words.replace(\"\\n\", \"\")\n",
    "                self.merge[tuple(words.strip().split())] = index\n",
    "            \n",
    "        self.reverseVocab = dict()\n",
    "        for i in self.vocab.keys():\n",
    "            self.reverseVocab[self.vocab[i]] = i\n",
    "    \n",
    "    def GetSplitWord(self, txt):        \n",
    "        txt = list(txt)\n",
    "        while(True):            \n",
    "            changeIndex = -1\n",
    "            rank = -1\n",
    "            for index in range(1, len(txt)):\n",
    "                tupl = (txt[index - 1], txt[index])\n",
    "                if(tupl in self.merge and (rank == -1 or (rank != -1 and self.merge[tupl] < rank))):\n",
    "                    changeIndex = index\n",
    "                    rank = self.merge[tupl]\n",
    "            if(changeIndex == -1):\n",
    "                break\n",
    "            txt[changeIndex-1] += txt[changeIndex]\n",
    "            txt.pop(changeIndex)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = txt.replace(\" \", \"Ġ\").replace(\"\\n\", \"Ċ\")\n",
    "        li = self.GetSplitWord(txt)\n",
    "        res = []\n",
    "        for word in li:            \n",
    "            res.append(self.vocab[word])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, li):\n",
    "        txt = \"\"\n",
    "        for i in li:\n",
    "            txt += self.reverseVocab[i]\n",
    "        return txt.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "145bfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMask(encd:list)->torch.tensor:\n",
    "    n = len(encd)\n",
    "    mask = torch.ones((n,n), device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\"))\n",
    "    mask = mask.tril()\n",
    "    return mask.unsqueeze(0)\n",
    "\n",
    "def SelectNextWord(prob):\n",
    "    sf = nn.Softmax(dim=-1)\n",
    "    prob = prob[-1] / 0.98\n",
    "    prob = sf(prob)\n",
    "    return torch.multinomial(prob, num_samples=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        dataSet = pd.read_csv(r\"C:\\Users\\shiva\\Desktop\\IISC\\code\\NeuroCpp\\Projects\\The Dream\\DataSet\\PeomWithTagDataSet.csv\")\n",
    "        self.poem = [' '.join(i.split()) for i in list(dataSet.Poem)]\n",
    "        self.tags = [' '.join(i.split()) for i in list(dataSet.Tags)]\n",
    "        self.model = gpt2()\n",
    "        self.tkn = tokenizer()\n",
    "        self.device = torch.device(deviceName if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # training \n",
    "        self.batchSize = 32\n",
    "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=1e-3)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def CreateData(self, index):\n",
    "        encode = self.tkn.encode(self.poem[index])\n",
    "        tags = self.tkn.encode(self.tags[index])\n",
    "        return encode, tags\n",
    "\n",
    "    def Train(self, epoch = 10):\n",
    "        for epc in range(epoch):\n",
    "            for i in range(0, len(self.poem), self.batchSize):\n",
    "                data, tags = self.CreateData(i)\n",
    "                totalLoss = 0\n",
    "                for k in range(len(data)):\n",
    "                    mask = CreateMask(tags)\n",
    "                    target = torch.tensor(data[k], device=self.device).unsqueeze(0)\n",
    "                    logit = self.model(torch.tensor(tags, device=self.device).view(1, len(tags)), mask)\n",
    "                    loss = self.loss_fn(logit[:,-1,:], target)\n",
    "                    totalLoss += loss\n",
    "                    tags.append(data[k])\n",
    "                print(f\"Epoch: {epc + 1}/{epoch} | Poem No.: {i + 1}/{len(self.poem)} | Loss: {totalLoss.item()}\")\n",
    "                totalLoss /= len(data)\n",
    "                totalLoss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                break\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "296a1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338806be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Poem No.: 1/13854 | Loss: 2448.75048828125\n"
     ]
    }
   ],
   "source": [
    "tr.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_14144\\3598555013.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = gptModel(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here lookhogLabourleanspick Mastery Breaker myster Yard Labourgive Album Breakertle romanticWomenwomenInterestoustPack LabourWomenwomenleans intimate TraderShip MasteryroundLabourLady Yardem Trader Contest Gentle tonShip YardDist weightsleansentleUMEem Warehouseem TraderWomenTrust myster\n"
     ]
    }
   ],
   "source": [
    "sent = \"Here look\"\n",
    "tkn = tokenizer()\n",
    "encd = tkn.encode(sent)\n",
    "itera = 1\n",
    "while(True):\n",
    "    x = torch.tensor(encd, device=deviceName)\n",
    "    mask = CreateMask(encd)\n",
    "    a = gpt2(torch.tensor(x).view(1,len(encd)), mask).squeeze(0)\n",
    "    encd.append(SelectNextWord(a))\n",
    "    itera += 1\n",
    "    if(itera > 50):\n",
    "        break\n",
    "print(tkn.decode(encd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
